<div align="center">
<br/>

<img src="https://lirp.cdn-website.com/445da6b2/dms3rep/multi/opt/DosePacker+and+DosePack+loogs-1-01-1920w.png" alt="DoseHack Logo" width="50"/>

# DoseHack: Team Zeros and Ones

</div>

## üì∏ Screenshots

## Simulation Environment Screenshots

![Screenshot 1](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-1.png?token=GHSAT0AAAAAACX74CG4DPFO5DVMD5WQARMAZXU3X5A)

![Screenshot 2](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-2.png?token=GHSAT0AAAAAACX74CG4PNYLQBOS3ROAQWBQZXU3Z2A)

![Screenshot 3](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-3.png?token=GHSAT0AAAAAACX74CG4ROHRBCNGCM72FQTEZXU32EA)

![Screenshot 4](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-4.png?token=GHSAT0AAAAAACX74CG5Z37E3KS42TELKXZSZXU32NQ)

![Screenshot 5](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-6.png?token=GHSAT0AAAAAACX74CG4WUUKMKH6ZE7M3PNKZXU33DA)

![Screenshot 6](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-7.png?token=GHSAT0AAAAAACX74CG5NAJWTQZZRGQBACCIZXU33PA)

![Screenshot 7](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-8.png?token=GHSAT0AAAAAACX74CG4GXMM62Q3AI72WCGSZXU333Q)

![Screenshot 8](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-9.png?token=GHSAT0AAAAAACX74CG4WDFZSSTLJJGOH2NYZXU34FA)

![Screenshot 9](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-11.png?token=GHSAT0AAAAAACX74CG5UHUHUODGI4I4CK6OZXU34UQ)

![Screenshot 10](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-13.png?token=GHSAT0AAAAAACX74CG5PANBNBCNFLAMO53WZXU35AA)

![Screenshot 11](https://raw.githubusercontent.com/DoseHack-24-2/Zeros_and_Ones/refs/heads/main/documentation/dosehack-14.png?token=GHSAT0AAAAAACX74CG4XC6VOAK6X74G5WHWZXU35KQ)

<div align="center">

[![Python](https://img.shields.io/badge/python-3670A0?style=for-the-badge&logo=python&logoColor=ffdd54)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)](https://pytorch.org/)
[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=for-the-badge&logo=numpy&logoColor=white)](https://numpy.org/)
[![Next.js](https://img.shields.io/badge/Next-black?style=for-the-badge&logo=next.js&logoColor=white)](https://nextjs.org/)
[![TailwindCSS](https://img.shields.io/badge/tailwindcss-%2338B2AC.svg?style=for-the-badge&logo=tailwind-css&logoColor=white)](https://tailwindcss.com/)
[![MongoDB](https://img.shields.io/badge/MongoDB-%234ea94b.svg?style=for-the-badge&logo=mongodb&logoColor=white)](https://www.mongodb.com/)
[![Postman](https://img.shields.io/badge/Postman-FF6C37?style=for-the-badge&logo=postman&logoColor=white)](https://www.postman.com/)

### Efficient Multi-Agent Path Planning for Warehouse Automation

[Live Demo](https://example.com/demo) | [Documentation](https://example.com/docs) | [Report Bug](https://example.com/report) | [Request Feature](https://example.com/feature)

</div>

## üìã Table of Contents

1. [üåü Introduction](#-introduction)
2. [üéØ Problem Statement](#-problem-statement)
3. [üí° Our Solution](#-our-solution)
4. [üîë Key Features](#-key-features)
5. [üîÑ System Workflow](#-system-workflow)
6. [‚öôÔ∏è Technology Stack](#Ô∏è-technology-stack)
7. [üöÄ Getting Started](#-getting-started)
8. [üñ•Ô∏è Installation](#Ô∏è-installation)
9. [üîß Configuration](#-configuration)
10. [üéÆ Usage](#-usage)
11. [üìä API Reference](#-api-reference)
12. [ü§ù Contributing](#-contributing)
13. [üìú License](#-license)
14. [üë• Team](#-team)
15. [üôè Acknowledgements](#-acknowledgements)
16. [üìû Contact](#-contact)
17. [üîÆ Future Roadmap](#-future-roadmap)

## üåü Introduction

AutoBotNavigator is an advanced multi-agent path planning system designed for warehouse environments. It leverages reinforcement learning techniques to efficiently guide multiple autobots across complex grid layouts, ensuring collision-free navigation and optimal path planning.

### Why AutoBotNavigator?

- **Efficiency**: Optimizes paths for multiple autobots simultaneously.
- **Collision Avoidance**: Ensures safe navigation in shared spaces.
- **Scalability**: Adapts to various warehouse layouts and bot quantities.
- **Real-time Coordination**: Dynamically adjusts paths to avoid conflicts.
- **Simplicity**: Utilizes simple movement commands for easy integration.

## üéØ Problem Statement

The challenge is to guide multiple autobots across a matrix grid from their starting points (A) to their destinations (B), avoiding obstacles and each other. Key challenges include:

1. Collision Avoidance
2. Dynamic Movement
3. Matrix Layout Complexity
4. Real-Time Coordination

## üí° Our Solution

AutoBotNavigator addresses these challenges through:

1. **Independent Q-Learning (IQL)**: Each autobot learns its optimal path independently.
2. **Prioritized Experience Replay**: Enhances learning from important experiences.
3. **Dueling Q-Network Architecture**: Improves value estimation for better decision-making.
4. **Dynamic Obstacle Handling**: Adapts to both static and moving obstacles.
5. **Parallel Action Execution**: Allows for simultaneous movement of multiple autobots.

## üîë Key Features

1. **Multi-Agent Reinforcement Learning**

   - Uses IQL for scalable multi-agent learning.
   - Each agent optimizes its path independently.

2. **Advanced Neural Network Architecture**

   - Implements Dueling Q-Network for better action-value estimation.
   - Separates state value and action advantage for more stable learning.

3. **Prioritized Experience Replay**

   - Focuses on important experiences to accelerate learning.
   - Adjusts sampling probability based on TD-error magnitude.

4. **Dynamic Environment Handling**

   - Adapts to changing obstacle positions.
   - Considers both static and moving obstacles in path planning.

5. **Collision Avoidance System**

   - Implements sophisticated logic to prevent inter-agent collisions.
   - Uses a combination of learned policies and rule-based safety checks.

6. **Flexible Grid Representation**

   - Supports various grid sizes and layouts.
   - Easily configurable for different warehouse setups.

7. **Performance Metrics**

   - Tracks total time and command count for optimization.
   - Provides insights into efficiency and areas for improvement.

8. **Visualization Tools**
   - Offers real-time visualization of agent movements.
   - Helps in understanding and debugging complex scenarios.

## üîÑ System Workflow

1. **Environment Initialization**:

   - Load the warehouse grid layout.
   - Initialize autobots at their starting positions.

2. **Training Phase**:

   - Agents explore the environment using Œµ-greedy policy.
   - Experiences are stored in prioritized replay buffer.
   - Neural networks are updated using sampled batches.

3. **Path Execution**:

   - Trained agents navigate from start to destination.
   - Real-time collision checking and avoidance.

4. **Performance Evaluation**:

   - Calculate total time and commands used.
   - Analyze efficiency and success rate.

5. **Iteration and Optimization**:
   - Fine-tune hyperparameters based on performance.
   - Retrain on more complex scenarios for robustness.

## ‚öôÔ∏è Technology Stack

- **Backend**:

  - Python 3.8+
  - PyTorch for deep learning
  - NumPy for numerical computations
  - Gym for reinforcement learning environments

- **Frontend**:

  - Next.js 14 for React-based web interface
  - Tailwind CSS for styling

- **Database**:

  - MongoDB for storing training data and results

- **API Testing**:

  - Postman for API development and testing

- **Deployment**:
  - Docker for containerization
  - Kubernetes for orchestration (planned)

## üöÄ Getting Started

These instructions will get you a copy of the project up and running on your local machine for development and testing purposes.

### Prerequisites

- Python 3.8+
- Node.js 14+
- MongoDB
- CUDA-capable GPU (recommended for training)

### üñ•Ô∏è Installation

1. Clone the repository

   ```
   git clone https://github.com/prodigyRahul/Zeros_and_ones.git
   cd Zeros_and_ones
   ```

2. Set up a virtual environment (optional but recommended)

   ```
   python -m venv venv
   source venv/bin/activate  # On Windows use `venv\Scripts\activate`
   ```

3. Install Python dependencies

   ```
   pip install -r requirements.txt
   ```

4. Install Node.js dependencies

   ```
   cd frontend
   npm install
   ```

5. Set up MongoDB
   (Provide specific instructions or scripts)

### üîß Configuration

1. Set up environment variables
   Create a `.env` file in the root directory and add:

   ```
   MONGO_URI=your_mongodb_connection_string
   ```

2. Configure the simulation parameters in `config.py`

## üéÆ Usage

1. Start the backend server

   ```
   python main.py
   ```

2. Start the frontend development server

   ```
   cd frontend
   npm run dev
   ```

3. Access the web interface at `http://localhost:3000`

## üìä API Reference

Detailed API documentation can be found in the [API.md](API.md) file.

## ü§ù Contributing

We welcome contributions to AutoBotNavigator! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) file for details on our code of conduct and the process for submitting pull requests.

## üìú License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

## üë• Team

Team Zeros and Ones:

- Rahul Mistry - Full Stack Developer
- Ronit Kothari - ML Engineer
- Abhishek Parmar - ML Engineer

## üôè Acknowledgements

- [OpenAI Gym](https://gym.openai.com/) for the reinforcement learning framework
- [PyTorch Team](https://pytorch.org/) for the deep learning library
- [Next.js](https://nextjs.org/) for the React framework

## üìû Contact

For support or queries, please email us at [rahulmistry.sde@gmail.com]

## üîÆ Future Roadmap

- Implement centralized multi-agent reinforcement learning approaches
- Develop more sophisticated reward shaping techniques
- Integrate with real-world warehouse management systems
- Enhance visualization tools for better interpretability
- Explore transfer learning for quick adaptation to new layouts

---

<div align="center">
Made with ‚ù§Ô∏è by Team Zeros and Ones
</div>
