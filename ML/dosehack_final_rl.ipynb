{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque, defaultdict\n",
        "import os\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Prioritized Experience Replay Buffer\n",
        "class PrioritizedReplayBuffer:\n",
        "    def __init__(self, buffer_size, batch_size, alpha=0.6):\n",
        "        self.buffer_size = buffer_size\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.buffer = []\n",
        "        self.pos = 0\n",
        "        self.priorities = np.zeros((buffer_size,), dtype=np.float32)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
        "        if len(self.buffer) < self.buffer_size:\n",
        "            self.buffer.append((state, action, reward, next_state, done))\n",
        "        else:\n",
        "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
        "        self.priorities[self.pos] = max_prio\n",
        "        self.pos = (self.pos + 1) % self.buffer_size\n",
        "\n",
        "    def sample(self, beta=0.4):\n",
        "        if len(self.buffer) == self.buffer_size:\n",
        "            prios = self.priorities\n",
        "        else:\n",
        "            prios = self.priorities[:self.pos]\n",
        "        probs = prios ** self.alpha\n",
        "        probs /= probs.sum()\n",
        "        indices = np.random.choice(len(self.buffer), self.batch_size, p=probs)\n",
        "        samples = [self.buffer[idx] for idx in indices]\n",
        "        total = len(self.buffer)\n",
        "        weights = (total * probs[indices]) ** (-beta)\n",
        "        weights /= weights.max()\n",
        "        batch = list(zip(*samples))\n",
        "        states, actions, rewards, next_states, dones = batch\n",
        "        return (\n",
        "            torch.FloatTensor(np.array(states)).to(device),\n",
        "            torch.LongTensor(actions).to(device),\n",
        "            torch.FloatTensor(rewards).to(device),\n",
        "            torch.FloatTensor(np.array(next_states)).to(device),\n",
        "            torch.FloatTensor(dones).to(device),\n",
        "            indices,\n",
        "            torch.FloatTensor(weights).to(device)\n",
        "        )\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        for idx, prio in zip(indices, priorities):\n",
        "            self.priorities[idx] = prio\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "# Dueling Q-Network\n",
        "class DuelingQNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DuelingQNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_size, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.value_stream = nn.Linear(64, 1)\n",
        "        self.advantage_stream = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        value = self.value_stream(x)\n",
        "        advantage = self.advantage_stream(x)\n",
        "        q_vals = value + (advantage - advantage.mean())\n",
        "        return q_vals\n",
        "\n",
        "# Independent Q-Learning Agent\n",
        "class IQLAgent:\n",
        "    def __init__(self, state_size, action_size, agent_id, lr=3e-4, gamma=0.99, tau=1e-3,\n",
        "                 buffer_size=100000, batch_size=64, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
        "        self.agent_id = agent_id\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        self.q_network = DuelingQNetwork(state_size, action_size).to(device)\n",
        "        self.target_network = DuelingQNetwork(state_size, action_size).to(device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "        self.target_network.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
        "        self.replay_buffer = PrioritizedReplayBuffer(buffer_size, batch_size)\n",
        "\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.999\n",
        "\n",
        "        self.beta = beta_start\n",
        "        self.beta_increment = (1.0 - beta_start) / beta_frames\n",
        "\n",
        "    def choose_action(self, state, evaluate=False):\n",
        "        if evaluate or random.random() > self.epsilon:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                q_values = self.q_network(state)\n",
        "            action = torch.argmax(q_values).item()\n",
        "        else:\n",
        "            action = random.randrange(self.action_size)\n",
        "        return action\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        self.replay_buffer.add(state, action, reward, next_state, done)\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "        for _ in range(2):\n",
        "            self.learn()\n",
        "\n",
        "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.replay_buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        states, actions, rewards, next_states, dones, indices, weights = self.replay_buffer.sample(self.beta)\n",
        "\n",
        "        q_values = self.q_network(states)\n",
        "        q_values_next = self.target_network(next_states)\n",
        "\n",
        "        target_q_values = rewards + (self.gamma * q_values_next.max(1)[0] * (1 - dones))\n",
        "\n",
        "        td_errors = target_q_values - q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        new_priorities = td_errors.abs().detach().cpu().numpy() + 1e-6\n",
        "        self.replay_buffer.update_priorities(indices, new_priorities)\n",
        "\n",
        "        loss = (td_errors ** 2 * weights).mean()\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.tau = min(1.0, self.tau + 0.01)\n",
        "        for target_param, local_param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
        "            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.q_network.state_dict(), path)\n",
        "\n",
        "    def load(self, path):\n",
        "        self.q_network.load_state_dict(torch.load(path))\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "# Warehouse Environment\n",
        "class WarehouseEnv(gym.Env):\n",
        "    def __init__(self, grid, num_autobots, max_steps=200):\n",
        "        super(WarehouseEnv, self).__init__()\n",
        "        self.grid = grid\n",
        "        self.num_autobots = num_autobots\n",
        "        self.max_steps = max_steps\n",
        "        self.current_step = 0\n",
        "        self.commands = defaultdict(int)\n",
        "\n",
        "        self.autobot_start = {}\n",
        "        self.autobot_end = {}\n",
        "        self.find_start_end_positions(['A1', 'A2'], ['B1', 'B2'])\n",
        "\n",
        "        self.obstacles = set()\n",
        "        self.find_obstacles(['X'])\n",
        "\n",
        "        self.moving_obstacles = set()\n",
        "        self.find_moving_obstacles(['M'])\n",
        "\n",
        "        self.action_space = gym.spaces.MultiDiscrete([5] * num_autobots)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0, high=max(len(grid), len(grid[0])),\n",
        "            shape=(num_autobots, 8), dtype=np.float32\n",
        "        )\n",
        "\n",
        "        self.autobot_positions = self.autobot_start.copy()\n",
        "        self.done = [False] * self.num_autobots\n",
        "\n",
        "    def find_start_end_positions(self, start_symbols, end_symbols):\n",
        "        for agent_id, symbol in enumerate(start_symbols):\n",
        "            found = False\n",
        "            for i, row in enumerate(self.grid):\n",
        "                for j, cell in enumerate(row):\n",
        "                    if cell == symbol:\n",
        "                        self.autobot_start[agent_id] = (i, j)\n",
        "                        found = True\n",
        "                        break\n",
        "                if found:\n",
        "                    break\n",
        "            if not found:\n",
        "                raise ValueError(f\"Start symbol {symbol} not found in grid.\")\n",
        "\n",
        "        for agent_id, symbol in enumerate(end_symbols):\n",
        "            found = False\n",
        "            for i, row in enumerate(self.grid):\n",
        "                for j, cell in enumerate(row):\n",
        "                    if cell == symbol:\n",
        "                        self.autobot_end[agent_id] = (i, j)\n",
        "                        found = True\n",
        "                        break\n",
        "                if found:\n",
        "                    break\n",
        "            if not found:\n",
        "                raise ValueError(f\"End symbol {symbol} not found in grid.\")\n",
        "\n",
        "    def find_obstacles(self, obstacle_symbols):\n",
        "        for symbol in obstacle_symbols:\n",
        "            for i, row in enumerate(self.grid):\n",
        "                for j, cell in enumerate(row):\n",
        "                    if cell == symbol:\n",
        "                        self.obstacles.add((i, j))\n",
        "\n",
        "    def find_moving_obstacles(self, moving_obstacle_symbols):\n",
        "        for symbol in moving_obstacle_symbols:\n",
        "            for i, row in enumerate(self.grid):\n",
        "                for j, cell in enumerate(row):\n",
        "                    if cell == symbol:\n",
        "                        self.moving_obstacles.add((i, j))\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.autobot_positions = self.autobot_start.copy()\n",
        "        self.done = [False] * self.num_autobots\n",
        "        return self.get_observation()\n",
        "\n",
        "    def get_observation(self):\n",
        "        obs = []\n",
        "        for agent_id in range(self.num_autobots):\n",
        "            x, y = self.autobot_positions[agent_id]\n",
        "            end_x, end_y = self.autobot_end[agent_id]\n",
        "            obs.append([x, y, end_x, end_y,\n",
        "                        self.commands[agent_id],\n",
        "                        self.done[agent_id],\n",
        "                        len(self.obstacles),\n",
        "                        len(self.moving_obstacles)])\n",
        "        return np.array(obs, dtype=np.float32)\n",
        "\n",
        "    def step(self, actions):\n",
        "        for agent_id in range(self.num_autobots):\n",
        "            if not self.done[agent_id]:\n",
        "                self.commands[agent_id] += 1\n",
        "                command = actions[agent_id]\n",
        "\n",
        "                if command == 0:  # Forward\n",
        "                    self.move_autobot(agent_id, 0, 1)\n",
        "                elif command == 1:  # Backward\n",
        "                    self.move_autobot(agent_id, 0, -1)\n",
        "                elif command == 2:  # Left\n",
        "                    self.move_autobot(agent_id, -1, 0)\n",
        "                elif command == 3:  # Right\n",
        "                    self.move_autobot(agent_id, 1, 0)\n",
        "                elif command == 4:  # Wait\n",
        "                    continue\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = all(self.done) or self.current_step >= self.max_steps\n",
        "        reward = self.get_reward(done)\n",
        "\n",
        "        return self.get_observation(), reward, done, {}\n",
        "\n",
        "    def move_autobot(self, agent_id, dx, dy):\n",
        "        x, y = self.autobot_positions[agent_id]\n",
        "        new_x, new_y = x + dx, y + dy\n",
        "        if (0 <= new_x < len(self.grid) and\n",
        "                0 <= new_y < len(self.grid[0]) and\n",
        "                (new_x, new_y) not in self.obstacles):\n",
        "            self.autobot_positions[agent_id] = (new_x, new_y)\n",
        "\n",
        "        if self.autobot_positions[agent_id] == self.autobot_end[agent_id]:\n",
        "            self.done[agent_id] = True\n",
        "\n",
        "    def get_reward(self, done):\n",
        "        if done and all(self.done):\n",
        "            return 100\n",
        "        elif done:\n",
        "            return -50\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "# Main Function to Train Agents\n",
        "def main():\n",
        "    grid = [\n",
        "        ['.', '.', '.', 'A1', '.', 'X'],\n",
        "        ['.', 'X', '.', '.', '.', '.'],\n",
        "        ['A2', '.', '.', 'X', '.', 'B1'],\n",
        "        ['.', '.', 'X', '.', '.', '.'],\n",
        "        ['B2', '.', '.', '.', '.', '.'],\n",
        "    ]\n",
        "\n",
        "    num_autobots = 2\n",
        "    env = WarehouseEnv(grid, num_autobots)\n",
        "\n",
        "    agents = [IQLAgent(state_size=8, action_size=5, agent_id=i) for i in range(num_autobots)]\n",
        "\n",
        "    num_episodes = 1000\n",
        "    for episode in range(num_episodes):\n",
        "        states = env.reset()\n",
        "        episode_reward = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            actions = [agent.choose_action(states[i]) for i, agent in enumerate(agents)]\n",
        "            next_states, rewards, done, _ = env.step(actions)\n",
        "            episode_reward += rewards\n",
        "\n",
        "            for i, agent in enumerate(agents):\n",
        "                agent.step(states[i], actions[i], rewards, next_states[i], done)\n",
        "\n",
        "            states = next_states\n",
        "\n",
        "        print(f\"Episode: {episode + 1}, Epsilon: {agents[0].epsilon:.2f}, Reward: {episode_reward:.2f}\")\n",
        "\n",
        "    # Save trained models   \n",
        "    os.makedirs(\"trained_models\", exist_ok=True)\n",
        "    for i, agent in enumerate(agents):\n",
        "        agent.save(f\"trained_models/agent_{i}.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
